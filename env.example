# Discord Bot Configuration
# Get these from https://discord.com/developers/applications
DISCORD_TOKEN=your_discord_bot_token_here
DISCORD_CLIENT_ID=your_discord_client_id_here

# Google Gemini AI Configuration
# Get this from https://makersuite.google.com/app/prompts/new_freeform
GEMINI_API_KEY=your_gemini_api_key_here

# Environment Configuration
NODE_ENV=production

# Core feature flags
# Enhanced intelligence enables MCP-backed features and related optimizations
ENABLE_ENHANCED_INTELLIGENCE=true
# Agentic intelligence enables agentic commands and behaviors
ENABLE_AGENTIC_INTELLIGENCE=true

# Built-in Analytics Dashboard
# This controls the internal lightweight dashboard server
ENABLE_ANALYTICS_DASHBOARD=true
ANALYTICS_DASHBOARD_PORT=3001

# Legacy/compat feature flags referenced by tests and legacy docs
# Note: ENABLE_ANALYTICS is NOT used by the runtime dashboard (use ENABLE_ANALYTICS_DASHBOARD above)
# and ENABLE_MCP_INTEGRATION is implied by ENABLE_ENHANCED_INTELLIGENCE.
ENABLE_ANALYTICS=true
ENABLE_MCP_INTEGRATION=true

# Enhanced Intelligence API Keys (for real MCP integration)
BRAVE_API_KEY=your_brave_search_api_key_here
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# Media & Speech Providers (optional)
# Stability AI for image generation
STABILITY_API_KEY=your_stability_ai_api_key_here
# Tenor for GIF search
TENOR_API_KEY=your_tenor_api_key_here
# ElevenLabs for TTS
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here
ELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM

# Agentic Intelligence Configuration
# These are used when agentic features are enabled
AGENTIC_CHANNELS=channel_id_1,channel_id_2
AGENTIC_ESCALATION_CHANNEL=moderator_channel_id
AGENTIC_MODERATOR_ROLES=moderator_role_id_1,moderator_role_id_2

# Database Configuration
# SQLite (Prisma) default for app data.
# For local dev, set to file:./prisma/dev.db. Docker Compose overrides to file:/data/dev.db at runtime.
DATABASE_URL=file:./prisma/dev.db

# Postgres (optional) for pgvector/vector store features.
# Do NOT set DATABASE_URL to Postgres unless you intentionally migrate Prisma to Postgres.
POSTGRES_URL=postgresql://chatterbot:chatterbot@postgres:5432/chatterbot
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=chatterbot
POSTGRES_USER=chatterbot
POSTGRES_PASSWORD=chatterbot
POSTGRES_SSL=false

# Health Check Configuration
HEALTH_CHECK_PORT=3000

# Logging Configuration
LOG_LEVEL=info

# Rate Limiting (Optional)
MAX_REQUESTS_PER_MINUTE=60
MAX_REQUESTS_PER_HOUR=1000

# Feature Flags (additional advanced features; optional)
ENABLE_MODERATION=true
ENABLE_MCP_INTEGRATION=true
# Advanced conversational capabilities
ENABLE_SELF_CRITIQUE=true
ENABLE_HYBRID_RETRIEVAL=true
ENABLE_WEB_GROUNDING=true
ENABLE_AUTO_MEMORY=true

# Ultra-Intelligence System Configuration
# Enables the complete ultra-intelligent Discord AI system
ENABLE_ULTRA_INTELLIGENCE=true

# Advanced Memory & Social Intelligence
ENABLE_EPISODIC_MEMORY=true
ENABLE_SOCIAL_INTELLIGENCE=true
ENABLE_EMOTIONAL_INTELLIGENCE=true
MAX_MEMORIES_PER_USER=1000
MEMORY_DECAY_RATE=0.01
MEMORY_IMPORTANCE_THRESHOLD=0.3
MEMORY_CONSOLIDATION_INTERVAL=3600000
SOCIAL_ANALYSIS_DEPTH=moderate
EMOTIONAL_SENSITIVITY=0.7
ADAPTATION_AGGRESSIVENESS=0.6

# Autonomous Reasoning Configuration
ENABLE_AUTONOMOUS_REASONING=true
AUTONOMOUS_REFLECTION_FREQUENCY=30
AUTONOMOUS_GOAL_EVALUATION_INTERVAL=60
AUTONOMOUS_PERSONA_ADAPTATION_THRESHOLD=0.7
AUTONOMOUS_MAX_ACTIVE_GOALS=5

# Ultra-Intelligent Research Configuration
ENABLE_ULTRA_RESEARCH=true
RESEARCH_CACHE_DURATION=7200000
RESEARCH_MAX_SOURCES=15
RESEARCH_VERIFICATION_LEVEL=standard

# Human-Like Conversation Configuration
ENABLE_HUMAN_CONVERSATION=true
CONVERSATION_ADAPTATION_SPEED=0.7
CONVERSATION_CREATIVITY_LEVEL=0.8
CONVERSATION_SOCIAL_AWARENESS=0.9
CONVERSATION_EXPERTISE_CONFIDENCE=0.8

# Ultra-Intelligence Behavioral Settings
ULTRA_INTELLIGENCE_PREFERRED_PERSONALITY=adaptive
ULTRA_INTELLIGENCE_MAX_PROCESSING_TIME=30000
ULTRA_INTELLIGENCE_ENABLE_REAL_TIME_LEARNING=true
ULTRA_INTELLIGENCE_ENABLE_PROACTIVE_INSIGHTS=true
ULTRA_INTELLIGENCE_ENABLE_MULTIMODAL_PROCESSING=true
ULTRA_INTELLIGENCE_ENABLE_SERVER_CULTURE_ADAPTATION=true
ULTRA_INTELLIGENCE_ENABLE_USER_RELATIONSHIP_MEMORY=true
ULTRA_INTELLIGENCE_ENABLE_CONTINUOUS_IMPROVEMENT=true

# Security Note: Documentation Updates
# Documentation updates should NOT be controlled by runtime environment variables
# as this creates potential security risks in production environments.
# Instead, use build/deployment scripts to update documentation.
# Example: Use CI/CD pipelines, build scripts, or deployment automation
# to handle documentation updates as part of the deployment process.

# Optional: Multi-provider model routing
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-latest
DEFAULT_PROVIDER=gemini

# Decision engine tuning (optional)
# Cooldown between replies per user (ms)
DECISION_COOLDOWN_MS=8000
# Model token limit used for strategy thresholds
DECISION_MODEL_TOKEN_LIMIT=8000
# Maximum total mentions (users+roles+channels) before reply is suppressed
DECISION_MAX_MENTIONS=6
# Ambient channel score threshold to reply when not DM/mention/reply
DECISION_AMBIENT_THRESHOLD=25
# Recent messages by user to treat as "burst"
DECISION_BURST_COUNT_THRESHOLD=3
# Minimum characters not considered "too short" in ambient
DECISION_SHORT_MSG_MIN_LEN=3

# Optional per-guild overrides (JSON mapping of guildId to DecisionEngineOptions)
# Example:
# DECISION_OVERRIDES_JSON={"123456789012345678":{"ambientThreshold":35,"maxMentionsAllowed":4},"234567890123456789":{"cooldownMs":6000}}
# TTL for DB-backed decision overrides cache (ms). Default 60000
# DECISION_OVERRIDES_TTL_MS=60000
# Optional: Comma-separated list to avoid selecting certain providers at runtime
# Useful if a provider lacks billing or is unstable in your region
DISALLOW_PROVIDERS=

# Additional Providers
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.1-70b-versatile
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest
OPENAI_COMPAT_API_KEY=your_openai_compatible_api_key_here
OPENAI_COMPAT_BASE_URL=https://your-openai-compatible-endpoint/v1
OPENAI_COMPAT_MODEL=qwen2.5-32b-instruct
# Optional convenience presets (used by model registry entries)
# You can point these at any OpenAI-compatible endpoint (OpenRouter, Together, local vLLM/Ollama compatible servers)
# Examples:
#  - OpenRouter base: https://openrouter.ai/api/v1
#  - Together base: https://api.together.xyz/v1
#  - Local vLLM: http://localhost:8000/v1
#  - Ollama (OpenAI compat proxy): http://localhost:11434/v1
OPENAI_COMPAT_MODEL_GEMMA3=google/gemma-3-27b-it
OPENAI_COMPAT_MODEL_GPT_OSS=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B

# Verification and Auto-Rerun
ENABLE_ANSWER_VERIFICATION=true
CROSS_MODEL_VERIFICATION=true
MAX_RERUNS=1

# Orchestration & AI SDK feature flags
FEATURE_TEMPORAL=true
FEATURE_VERCEL_AI=true
FEATURE_PGVECTOR=true
FEATURE_LANGGRAPH=true
FEATURE_OPENAI_RESPONSES=true
FEATURE_OPENAI_RESPONSES_TOOLS=true
FEATURE_TOOL_SUMMARY=true
FEATURE_RERANK=true
FEATURE_PERSIST_TELEMETRY=true
COST_TIER_MAX=medium
SPEED_TIER_MIN=medium
AI_API_KEY=your_vercel_ai_sdk_key_here
# Temporal Orchestration
TEMPORAL_TASK_QUEUE=discord-ai-bot
TEMPORAL_ADDRESS=127.0.0.1:7233
# Maintenance intervals
MEMORY_CONSOLIDATION_INTERVAL=3600000
VECTOR_MAINTENANCE_INTERVAL=21600000
KB_CHUNK_TTL_DAYS=180

# Optional: Cohere for reranking
COHERE_API_KEY=your_cohere_api_key_here
# Cohere rerank model (default: rerank-english-v3.0 or rerank-3.5-mini depending on SDK)
COHERE_RERANK_MODEL=rerank-english-v3.0

# Optional: Provider preference can be set per-request via internal routing; `/chat` is for initial opt-in only.

# Edge AI deployment
EDGE_MAX_NODES=5
EDGE_LOAD_THRESHOLD=0.8
EDGE_SYNC_INTERVAL_MS=30000
EDGE_FAILOVER_ENABLED=true
EDGE_MODEL_REPLICATION=2
EDGE_UPTIME_SUCCESS_RATE=0.95
EDGE_MAX_SIMULATED_LOAD_FACTOR=0.9

# Streaming/Auth
# Real-time service expects a token starting with 'user_' via query (?token=) or Authorization: Bearer <token>

# OpenTelemetry exporter endpoint
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/traces

# Helicone (optional proxy)
HELICONE_BASE_URL=https://oai.helicone.ai/v1
HELICONE_API_KEY=
HELICONE_CACHE_ENABLED=false
HELICONE_CACHE_MAX_AGE=300

# Langfuse (optional telemetry)
FEATURE_LANGFUSE=true
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
LANGFUSE_BASE_URL=https://cloud.langfuse.com

# Precise tokenization (optional)
FEATURE_PRECISE_TOKENIZER=true
TOKENIZER_ENCODING=cl100k_base

# Semantic cache (optional)
FEATURE_SEMANTIC_CACHE=true
REDIS_URL=redis://localhost:6379
FEATURE_SEMANTIC_CACHE_PERSIST=true
SEMANTIC_CACHE_TTL_MS=300000
SEMANTIC_CACHE_DISTANCE=0.9
SEMANTIC_CACHE_MAX_ENTRIES=200
SEMANTIC_CACHE_EMBEDDING_MODEL=text-embedding-3-small

# Token-budget guardrails (optional)
# When enabled, the router trims history and truncates overly long prompts to fit model context
# while reserving some output tokens. Uses precise tokenizer if FEATURE_PRECISE_TOKENIZER=true.
FEATURE_TOKEN_GUARDRAILS=true
TOKEN_GUARD_INPUT_BUDGET_FRACTION=0.7
TOKEN_GUARD_OUTPUT_RESERVE=512

# Voyage reranker (optional)
VOYAGE_API_KEY=
VOYAGE_RERANK_MODEL=rerank-2.5

# Reranker provider preference: cohere | voyage | auto
RERANK_PROVIDER=auto

# Local reranker (optional, OSS)
# When enabled, uses Transformers.js with a small Sentence Transformers model to re-rank candidates locally.
# Note: This runs on CPU by default; for servers with constrained resources, keep disabled.
FEATURE_LOCAL_RERANK=true
LOCAL_RERANK_MODEL=Xenova/all-MiniLM-L6-v2

# Enhanced Research Features - Phase 1: Core Infrastructure 
FEATURE_ENHANCED_LANGFUSE=true
FEATURE_MULTI_PROVIDER_TOKENIZATION=true
FEATURE_SEMANTIC_CACHE_ENHANCED=true
FEATURE_LANGGRAPH_WORKFLOWS=true

# Enhanced Research Features - Phase 2: Vector & Database
FEATURE_QDRANT_VECTOR_DB=true
FEATURE_VECTOR_MIGRATION_UTILS=true
FEATURE_ADVANCED_FILTERING=true

# Enhanced Research Features - Phase 3: Web & Accessibility
FEATURE_CRAWL4AI_WEB_ACCESS=true
FEATURE_WEB_CONTENT_EXTRACTION=true
FEATURE_URL_PROCESSING_SERVICE=true

# Enhanced Research Features - Phase 4: Multimodal
FEATURE_QWEN25VL_MULTIMODAL=true
FEATURE_IMAGE_ANALYSIS_SERVICE=true
FEATURE_VISUAL_REASONING=true
FEATURE_MULTIMODAL_CONTEXT=true

# Enhanced Research Features - Phase 5: Knowledge Graphs
FEATURE_KNOWLEDGE_GRAPHS=true
FEATURE_GRAPH_BASED_REASONING=true
FEATURE_ENTITY_RELATIONSHIP_TRACKING=true
FEATURE_GRAPH_QUERY_CAPABILITIES=true

# Enhanced Research Features - Phase 6: DSPy RAG Optimization
FEATURE_DSPY_RAG_OPTIMIZATION=true
FEATURE_AUTOMATIC_PROMPT_TUNING=true
FEATURE_FEW_SHOT_OPTIMIZATION=true
FEATURE_OPTIMIZED_RETRIEVAL=true

# Enhanced Research Features - Phase 7: Advanced AI Features
FEATURE_AUTONOMOUS_TOOL_GENERATION=true
FEATURE_CAUSAL_REASONING_ENGINE=true
FEATURE_CONTEXT_AWARE_RESPONSES=true
FEATURE_PERFORMANCE_OPTIMIZATION=true

# Performance Monitoring and Analytics
ENABLE_PERFORMANCE_MONITORING=true
PERFORMANCE_MONITORING_ALERT_THRESHOLDS_HIGH_LATENCY_MS=5000
PERFORMANCE_MONITORING_ALERT_THRESHOLDS_CRITICAL_LATENCY_MS=10000
PERFORMANCE_MONITORING_ALERT_THRESHOLDS_HIGH_ERROR_RATE=0.15
PERFORMANCE_MONITORING_ALERT_THRESHOLDS_CRITICAL_ERROR_RATE=0.30
PERFORMANCE_MONITORING_MAX_METRICS_HISTORY=10000
PERFORMANCE_MONITORING_CLEANUP_INTERVAL_HOURS=24
PERFORMANCE_MONITORING_ALERT_CHECK_INTERVAL_MINUTES=5

# Eval harness (Promptfoo)
# Set OPENAI_API_KEY to enable CI/local evals; without it, the workflow skips gracefully.
